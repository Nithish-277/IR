{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Modules\n",
    "\n",
    "import nltk                            # Importing nltk to download list of stopwords\n",
    "import os                              # Importing os to get paths from system\n",
    "from nltk.corpus import stopwords      # Importing stopWords\n",
    "import numpy as np                     # numpy module for mathematical calculations\n",
    "from collections import OrderedDict    # library to sort dictionary\n",
    "from nltk.stem import PorterStemmer    # PorterStemmer to perform Stemming\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECLARING GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring Global variables\n",
    "\n",
    "stopwords = set(stopwords.words('english'))                            #stopwords\n",
    "\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~''' + '\"'               #punctuations\n",
    "\n",
    "path = os.getcwd()                                                     #getting current working directory\n",
    "\n",
    "path = path + '\\\\data'                                           #getting the path of dataset\n",
    "\n",
    "dir_list = os.listdir(path)   # getting all files in a folder in sepcified path\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "invertedIndex = {}      # Dictionary for Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_char = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
    "             'u','v', 'w', 'x', 'y','z','0','1','2','3','4','5','6','7','8','9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file):\n",
    "    tokens = file.split('\\n')\n",
    "    tokens = tokens[0]\n",
    "    final_desc = ''\n",
    "    for x in tokens.lower():\n",
    "        if x not in valid_char:\n",
    "            final_desc+= ' '\n",
    "        else:\n",
    "            final_desc+=x\n",
    "    final_tokens = []\n",
    "    for w in final_desc.split(' '):\n",
    "        w = ps.stem(w)\n",
    "        if w not in stopwords and w!=' ' and w!='' and w not in valid_char:\n",
    "            final_tokens.append(w)\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING INVERTED INDEX\n",
    "    Logarithmic term frequency in for each term in each document is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Inverted_Index\n",
    "\n",
    "# Traversing Each File to create Inverted Index\n",
    "i = 0\n",
    "tf_docs = {}\n",
    "tf_idf_scr = {}\n",
    "for file in dir_list:\n",
    "    print(i)\n",
    "    f = path + '\\\\' + file\n",
    "    docID = file[:-4]\n",
    "    tf_docs[docID] = {}\n",
    "    tf_idf_scr[docID] = {}\n",
    "    file = open(f,encoding = 'utf-8').read()\n",
    "    tokens = preprocess(file)  # Calling Preprocessing function to get tokens\n",
    "    for token in tokens:                                 # Creating Posting_List for each index\n",
    "        if token not in invertedIndex.keys():\n",
    "            invertedIndex[token] = {}\n",
    "            invertedIndex[token][docID] = 1\n",
    "        elif docID not in invertedIndex[token].keys():\n",
    "            invertedIndex[token][docID] = 1 \n",
    "        elif docID in invertedIndex[token].keys():\n",
    "            invertedIndex[token][docID] = invertedIndex[token][docID] + 1\n",
    "        tf_docs[docID][token] = invertedIndex[token][docID]\n",
    "        if tf_docs[docID][token]:\n",
    "            tf_idf_scr[docID][token] = 1 + np.log2(tf_docs[docID][token])\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITING INVERTED INDEX IN TEXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('newIndex.txt', 'w', encoding = 'utf-8') as f: \n",
    "    for key, value in invertedIndex.items():\n",
    "        posting_list = \"{\"\n",
    "        for pkey,pvalue in value.items():\n",
    "            if pkey!=list(value.keys())[-1]:\n",
    "                posting_list = str(posting_list) + str(pkey) + \"=\" + str(pvalue) + \",\"\n",
    "            else:\n",
    "                posting_list = str(posting_list) + str(pkey) + \"=\" + str(pvalue)\n",
    "        posting_list = posting_list + \"}\"\n",
    "        f.write('{}({})==>{}\\n\\n\\n\\n\\n\\n\\n'.format(key, len(value),posting_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"invertedIndex.pickle\",\"wb\")\n",
    "pickle.dump(invertedIndex,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('invertedIndex.pickle','rb') as f:\n",
    "    invertedIndex = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(invertedIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCULATING DOCUMENT FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "for word in invertedIndex.keys():\n",
    "    df[word] = len(invertedIndex[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCULATING INVERSE DOCUMENT FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = {}\n",
    "M = len(invertedIndex)\n",
    "for word in invertedIndex.keys():\n",
    "    idf[word] = np.log2((M+1) / df[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCULATING TF-IDF SCORES FOR ALL TERMS IN VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in tf_idf_scr.items():\n",
    "    for tkey,tvaue in value.items():\n",
    "        tf_idf_scr[key][tkey]*=idf[tkey]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING VECTOR SPACE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorSpaceModel(query,tfidf_scr):\n",
    "    query_vocab = preprocess(query)\n",
    "    query_wc = {}\n",
    "    query_list = query.split(' ')\n",
    "    for i in range(len(query_list)):\n",
    "        query_list[i] = ps.stem(query_list[i])\n",
    "    for word in query_vocab:\n",
    "        query_wc[word] = query_list.count(word)\n",
    "    relevance_scores = {}\n",
    "    for doc_id in tfidf_scr.keys():\n",
    "        score = 0\n",
    "        # finding dot product between query and document\n",
    "        for word in query_vocab:\n",
    "            if word in invertedIndex.keys() and word in tfidf_scr[doc_id].keys():\n",
    "                score += query_wc[word] * tfidf_scr[doc_id][word]\n",
    "            else:\n",
    "                score = 0\n",
    "        relevance_scores[doc_id] = score\n",
    "    sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))\n",
    "    top_10 = {k: sorted_value[k] for k in list(sorted_value)[:10]}\n",
    "    return top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = 'Dell Inspiron laptop'\n",
    "query2 = 'Playstation'\n",
    "query3 = 'Nintendo switches'\n",
    "query4 = 'shirts and pant'\n",
    "query5 = 'real me pro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1 = vectorSpaceModel(query1,tf_idf_scr)    #returns top 5 documents using VSM\n",
    "top2 = vectorSpaceModel(query2,tf_idf_scr)    #returns top 5 documents using VSM\n",
    "top3 = vectorSpaceModel(query3,tf_idf_scr)    #returns top 5 documents using VSM\n",
    "top4 = vectorSpaceModel(query4,tf_idf_scr)    #returns top 5 documents using VSM\n",
    "top5 = vectorSpaceModel(query5,tf_idf_scr)    #returns top 5 documents using VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = []\n",
    "for key in top2.keys():\n",
    "    relevant_docs.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in relevant_docs:\n",
    "    print('Document Number = ' + file)\n",
    "    f = path + '\\\\' + file + '.txt'\n",
    "    content = open(f,encoding = 'utf-8').read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Best Seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_factors = {\n",
    "    '4.5-5.0' : 1,\n",
    "    '4.0-4.5' : 0.9,\n",
    "    '3.5-4.0' : 0.8,\n",
    "    '3.0-3.5' : 0.7,\n",
    "    '2.5-3.0' : 0.6,\n",
    "    '2.0-2.5' : 0.5,\n",
    "    '1.5-2.0' : 0.4,\n",
    "    '1.0-1.5' : 0.3,\n",
    "    '0.5-1.0' : 0.2,\n",
    "    '0.0-0.5' : 0.1,\n",
    "    '-1' : -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestSellProduct(rel_docs):\n",
    "    best_sell = []\n",
    "    factors = {}\n",
    "    for doc in rel_docs:\n",
    "        factors[doc] = []\n",
    "        fp = open(path + '\\\\' + doc + str('.txt'),encoding = 'utf-8')\n",
    "        for i, line in enumerate(fp):\n",
    "            if i == 2:\n",
    "                if line[:3]!='':\n",
    "                    factors[doc].append(line[:3])\n",
    "                else:\n",
    "                    factors[doc].append(-1)\n",
    "            elif i == 3:\n",
    "                if line!='':\n",
    "                    factors[doc].append(line)\n",
    "                else:\n",
    "                    factors[doc].append(-1)\n",
    "            elif i > 3:\n",
    "                break\n",
    "        fp.close()\n",
    "        for key,value in factors.items():\n",
    "            if value[0]!='\\n':\n",
    "                value[0] = float(value[0])\n",
    "            else:\n",
    "                value[0] = -1\n",
    "            if value[1]!='\\n':\n",
    "                value[1] = int(value[1][:-1])\n",
    "            else:\n",
    "                value[1] = -1\n",
    "        basis_factors = {}\n",
    "        for key,value in factors.items():\n",
    "            if value[0]<0 or value[1] < 0:\n",
    "                basis_factors[key] = -1\n",
    "            else:\n",
    "                if np.ceil(value[0])-value[0]>0.5:\n",
    "                    m_factor = str(np.floor(value[0])) + '-' + str(np.floor(value[0])+0.5)\n",
    "                else:\n",
    "                    m_factor = str(np.ceil(value[0])-0.5) + '-' + str(np.ceil(value[0]))\n",
    "                rating_range = m_factor\n",
    "                basis_factors[key] = np.multiply(value[0],value[1])*m_factors[rating_range]\n",
    "        values = list(basis_factors.values())\n",
    "        value = np.max(values)\n",
    "        for key,val in basis_factors.items():\n",
    "            if val == value:\n",
    "                best_sell.append(key)\n",
    "                break\n",
    "        return best_sell[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Few Examples Queries\\n')\n",
    "print('1)laptop')\n",
    "print('2)Playstation')\n",
    "print('3)Nintendo switches')\n",
    "print('4)shirts and pant')\n",
    "print('5)real me pro')\n",
    "query = input('ENTER THE QUERY')\n",
    "rdocs = vectorSpaceModel(query,tf_idf_scr,)\n",
    "bestSellProduct = findBestSellProduct(rdocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing Best Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bestSellProduct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = bestSellProduct\n",
    "print('Document Number = ' + file)\n",
    "f = path + '\\\\' + file + '.txt'\n",
    "content = open(f,encoding = 'utf-8').read()\n",
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
